#!/usr/bin/python
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Alphabet import IUPAC
from Bio.SeqFeature import SeqFeature, FeatureLocation
from Bio import Restriction
from Bio.Restriction import *
from Bio.Seq import Seq
from Bio.Alphabet import generic_dna, generic_protein
from Bio.SeqUtils import MeltingTemp as mt
from subprocess import call
from StringIO import StringIO
import os
import sys
import copy
import time
import csv
import pymysql.cursors

# To run the cript:
# ./generate_cluster_fragments_v5.py backbones.fasta 08_CP004121/ 08_CP004121/CP004121_ORFs.fasta CP004121
# v2working for cluster #5.

TRIM_SIZE = 1800  # maximum allowable length of a gene for ordering from Twist
OVERHANG_LEN = 60
#OPERON_SIZE = 6000
OPERON_SIZE = 11000  # increaseo peron size when you want to specify the order of the genetic parts
db = pymysql.connect(host='localhost',user='root',passwd='HiMommy12')
cursor = db.cursor()
query = ("SHOW DATABASES")
cursor.execute(query)
for r in cursor:
    print r

# Use this function to chop up sequences and add name to each chopped fragment
def split_seq(this_seq, seq_name, trim_size, oh_length):
    seq = str(this_seq)
    # print "sequence to chop", seq_name, "\n", seq
    split_seqs_list = set()
    seq_len = len(seq)
    # if the whole sequence length is smaller than the trim size, return the same sequence
    if seq_len < trim_size:
        split_seqs_list = [(seq_name, seq)]
    else:
        split_num = int((seq_len - oh_length - 1) / (trim_size - oh_length)) + 1
        lfrag = int((seq_len - oh_length) / split_num)
        print seq_len, split_num, lfrag, oh_length
        for i in range(0, split_num):
            this_start = i * lfrag
            this_end = (i + 1) * lfrag + oh_length
            if i == split_num - 1:
                this_end = seq_len
            split_seqs_list.add((seq_name + "_" + str(i), seq[this_start: this_end]))
            elements_to_seq[(seq_name + "_" + str(i))] = seq[this_start: this_end]
            print this_start, this_end
    return split_seqs_list


# Convert fasta file into genbank
def fasta_to_genbank(plasmid, cluster_name, cluster_dir):

    # Create a sequence
    seq = get_seq(plasmid)
    # Asign generic_dna or generic_protein
    sequence_object = Seq(str(seq), IUPAC.unambiguous_dna)

    # Create a record
    record = SeqRecord(sequence_object,
                       id='123456789',  # random accession number
                       name=cluster_name,
                       description='An example GenBank file generated by GNPN')
    print "\n"
    # Add annotation
    for element in plasmid:
        element_start = seq.find(str(elements_to_seq[element]))
        element_end = element_start + len(elements_to_seq[element])
        feature = SeqFeature(FeatureLocation(start=element_start, end=element_end), type='CDS', qualifiers={'locus_tag': element})
        print element, element_start + 1, element_end
        record.features.append(feature)
    # print record.features

    # Save as GenBank file
    if 'pETDuet_backbone' in plasmid:
        backbone_name = 'pETDuet'
    if 'pRSFDuet_backbone' in plasmid:
        backbone_name = 'pRSFDuet'
    gbfile_name = cluster_name + "-" + time.strftime("%H:%M:%S") + ".gb"
    output_file = open(os.path.join(cluster_dir, gbfile_name), "w")
    SeqIO.write(record, output_file, 'genbank')
TerminatorDic = csv.DictReader(open('Terminator.csv'))
PromoterDic = csv.DictReader(open('Promoter.csv'))
    

elements_to_seq = {"T7_tetO": "taatacgactcactataggTCTATCATTGATAGGgtttccctctagata",
                   "UTRB": "tctagaaataattttgtttaactttaagaaggagatatacc",
                   "UTRA": "ataatttaaaaaacagacctcatatcgaaataaaagaaggagatatacc",
                   "UTRd1": "tgtagaaataattttgtttaactttaataaggagatatacc",
                   "UTRd2": "atcttagtatattagttaagtataagaaggagatatacata",
                   "insulator_wt": "agggctccggactgcgctgtatagt",
                   "insulator_m6": "cccgtacgcgagataaactgctagg",
                   "pheA1": "gacgaacaaTAAGGCCTCCCAAATCGGGGGGCCTTTTTTATTgaTaacaaaa",
                   "ECK120033737": "ggaaacacagAAAAAAGCCCGCACCTGACAGTGCGGGCTTTTTTTTTcgaccaaagg",
                   "ECK120029600": "TTCAGCCAAAAAACTTAAGACCGCCGGTCTTGTCCACTACCTTGCAGTAATGCGGTGGACAGGATCGGCGGTTTTCTTTTCTCTTCTCAA",
                   "NotI": "GCGGCCGC",
                   "SrfI": "GCCCGGGC",
                   "SwaI": "ATTTAAAT",
                   "PmlI": "CACGTG",
                   "PmeI": "GTTTAAAC",
                   "SalI": "GTCGAC",
                   "SexAI": "ACCAGGT",
                   "SgrAI": "CACCGGCG",
                   "NheI": "GCTAGC",
                   "HindIII": "AAGCTT",
                   "AscI": "GGCGCGCC",
                   "SpeI": "ACTAGT",
                   "SceI_(I-SceI)": "TAGGGATAACAGGGTAAT",
                   "ura3_extended_upoh": "tgtaagcggatgccgggagcagacaagcccgtcagggcgcgtcagcgggtgttggcgggt",
                   "ura3_extended_downoh": "TATTACCCTATGCGGTGTGAAATACCGCACAGATGCGTAAGGAGAAAATACCGCATCAGG"}


prom_list = ["pSOD1",
"pFBA1",
"pCCW12",
"pTSA1",
"pZEO1",
"pNCE102",
"pHHF1",
"pSED1",
"pCPR1",
"pPMA1",
"pTDH3",
"pRPS20",
"pACS1",
"pSFC1",
"pNCA3",
"pJEN1",
"pPCK1",
"pFAT3",
"pPUT1",
"pIDP2",
"pYLR307C-A",
"pYLR312C",
"pFBP1",
"pCAT2",
"pSIP18",
"pADH2_cerevisiae",
"pMLS1",
"pALP1",
"pATO2",
"pCRC1",
"pGAC1",
"pPUT4",
"pCIT3",
"pPDH1",
"pYIG1",
"pICL2",
"pECM13",
"pREG2",
"pPHO89",
"pADY2",
"pNDE2",
"pSTL1",
"pCTA1",
"pICL1",
"pAGX1",
"pNQM1",

"pYGR067C",
"pRGI2",
"pINA1",
"pHXT2",

"pYOL014W",
"pHXT3",
"pIRC7",
"pTPO2",
"pADH2_bayanus",
"pADH2_castellii",
"pADH2_kudriavzevii",
"pADH2_mikitae",
"pADH2_paradoxus",
"pADH2_cerevisiae_flag",
"pPYK1",
"pTEF1",
"pTEF1_flag",
"pTEF2",
"pTPI1",
"pPDC1",
"pAlcA_AN",
"pAmyB_AN",
"pGlaA_AN",
"pGpdA_AN",
"pMbfA_AN",
"pTef1_AN",
"pTrpC_AN",
"pGlaA_AN_flag",
"plCL1"]


utr_list = ["UTRB", "UTRA", "UTRd1", "UTRd2"]
term_list = ["ECK120029600", "ECK120033737", "pheA1"]
ins_list = ["insulator_wt", "insulator_m6"]
re_list = ["NotI", "SrfI", "SwaI", "PmlI", "PmeI", "SalI", "SexAI", "SgrAI", "NheI", "HindIII", "AscI", "SpeI", "SceI_(I-SceI)"]

# Input fasta file of backbones and ura3
backbones_list_file = sys.argv[1]
# backbone_name_to_seq = {}
backbone_list = []
for record in SeqIO.parse(backbones_list_file, "fasta"):
    seq_name = record.id
    seq = record.seq
    # add backbone sequences to dictionary with all genetic elements
    elements_to_seq[seq_name] = seq
    backbone_list.append(seq_name)

# Input fasta file of PKS orfs:
cluster_dir = sys.argv[2]
orf_list_file = sys.argv[3]
cluster_name = sys.argv[4]
total_cluster_len = 0
orf_list = []
name_to_seq = {}  # need this dict to calculate size of operon

for record in SeqIO.parse(orf_list_file, "fasta"):
    seq_name = record.id
    seq = record.seq
    seq_len = len(record.seq)
    total_cluster_len += seq_len
    print seq_name, seq_len, len(seq), seq[:10]
    elements_to_seq[seq_name] = seq
    name_to_seq[seq_name] = seq  # need this dict to calculate size of operon
    orf_list.append(seq_name)

orf_list_sorted = copy.copy(orf_list)

num_genes = len(orf_list)
print "\nTotal clulster len", total_cluster_len
num_operons = (total_cluster_len / OPERON_SIZE) + 1
print "Number Operons %s\n" % num_operons

if num_operons > 4:
    print "increase size of operon"
    sys.exit(1)

orf_list.sort(key=lambda x: len(name_to_seq[x]), reverse=True)
print "Selected as first on each operon %s\n" % orf_list[0:num_operons]

used_orf = set()
for el in orf_list[0:num_operons]:
    used_orf.add(el)


def get_promotor_gen():
    index = 0
    while True:
        yield prom_list[index]
        index += 1
        index = index % len(prom_list)

promotor_gen = get_promotor_gen()


def get_term_gen():
    index = 0
    while True:
        yield term_list[index]
        index += 1
        index = index % len(term_list)

term_gen = get_term_gen()


def get_utr_gen():
    index = 0
    while True:
        yield utr_list[index]
        index += 1
        index = index % len(utr_list)

utr_gen = get_utr_gen()


def get_ins_gen():
    index = 0
    while True:
        yield ins_list[index]
        index += 1
        index = index % len(ins_list)

ins_gen = get_ins_gen()


def get_operon_size(operon):
    size = 0
    for o in operon:
        if o in name_to_seq:
            size += len(name_to_seq[o])
        elif o in elements_to_seq:
            size += len(elements_to_seq[o])
        # print o, size
    return size


def get_gene(size_left):
    for o in orf_list:
        if o in used_orf:
            continue
        if len(name_to_seq[o]) < size_left:
            return o
    return None


def construct_operon(initial_element):
    operon = []
    operon.append(next(promotor_gen))
    operon.append(next(utr_gen))
    operon.append(initial_element)
    counter_orfs = 1
    operon_size_left = OPERON_SIZE - get_operon_size(operon)
    while operon_size_left > 0:
        # print operon_size_left
        next_gene = get_gene(operon_size_left)
        # print next_gene
        if not next_gene:
            break

        operon.append(next(utr_gen))
        operon.append(next_gene)
        counter_orfs += 1
        used_orf.add(next_gene)
        operon_size_left = OPERON_SIZE - get_operon_size(operon)
        if counter_orfs > 4:
            break
    operon.append(next(term_gen))
    # print counter_orfs
    return operon


operons = []
for operon_id in xrange(num_operons):
    operon = construct_operon(orf_list[operon_id])
    operons.append(operon)
    print "Operon %s has size %s : %s" % (operon_id, get_operon_size(operon), operon)

# plasmids = []
# for i in xrange(0, num_operons, 2):
#     plasmid = []
#     plasmid.append(backbone_list[i / 2])
#     plasmid += operons[i]
#     plasmid.append(backbone_list[-1])
#     plasmid.append(next(ins_gen))
#     plasmid += operons[i + 1]
#     print "Plasmid %s" % plasmid
#     plasmids.append(plasmid)

# USed for cluster 8 CP004121
# For like super special cases where I want to specify which operon to be added to which backbone:

plasmids = []
plasmid1 = []
plasmid2 = []
plasmid1.append(backbone_list[0])
plasmid1 += operons[1]
plasmid1.append(backbone_list[-1])
print len(operons)
plasmid1 += operons[2]
plasmids.append(plasmid1)
plasmid2.append(backbone_list[1])
plasmid2 += operons[0]
plasmid2.append(backbone_list[-1])
plasmid2 += operons[3]
plasmids.append(plasmid2)


# For clusters with much longer ORFS specify plasmids Manually and increase size of operon to 10000:
# 07_CP001966:
# plasmid1 = ['pETDuet_backbone', 'T7_tetO', 'UTRd1', 'Tpau_3476', 'UTRB', 'GV_PCCB', 'UTRA', 'GV_PCCA', 'pheA1', 'ura3_extended', 'insulator_m6', 'T7_tetO', 'UTRd1', 'Tpau_3478', 'UTRd2', 'Tpau_3479', 'UTRB', 'Tpau_3474', 'UTRA', 'Tpau_3475', 'ECK120029600']
# plasmid2 = ['pRSFDuet_backbone', 'T7_tetO', 'UTRB', 'Tpau_3480', 'ECK120029600', 'ura3_extended', 'insulator_wt', 'T7_tetO', 'UTRA', 'Tpau_3477', 'ECK120033737']
# plasmids = [plasmid1, plasmid2]

print "\nplamsid1", len(plasmids[0]), plasmids[0]
print "\nplamsid2", len(plasmids[1]), plasmids[1]


def get_seq(plasmid):
    seq = ""
    for p in plasmid:
        seq += str(elements_to_seq[p])
    return seq

full_seq = ''.join(get_seq(plasmid) for plasmid in plasmids)

# Search for restriction enzymes in sequences and add them
allowed_re = []

for re in re_list:
    re_seq = elements_to_seq[re].lower()
    construct_seq = full_seq.lower()
    # print construct_seq
    print re, construct_seq.find(re_seq), "in the whole sequence"
    if construct_seq.find(re_seq) == -1:
        allowed_re.append(re)

if len(allowed_re) < 4:
    print "Add more enzymes to search for in the sequence"
allowed_re = sorted(allowed_re)
print "\nAllowed restriction enzymes: ", sorted(allowed_re)

re_index = 0

# Store plasmid sequences in list
plasmid_seqs = []

for plasmid_id, plasmid in enumerate(plasmids):
    print "plasmid id", plasmid_id
    re_index = 0
    new_plasmid = []
    for item in plasmid:
        if item in prom_list or item in term_list:
            new_plasmid.append(allowed_re[re_index])
            re_index += 1
        new_plasmid.append(item)
        if item in prom_list:
            new_plasmid.append(allowed_re[re_index])

    plasmids[plasmid_id] = new_plasmid
    print new_plasmid

    # Concatenate all seqeuce elements into two pET and pRSF constructs
    plasmid_seqs.append(get_seq(new_plasmid))

# Write in files
# cluster_dir = cluster_name
# if not os.path.exists(cluster_dir):
#     os.makedirs(cluster_dir)


# sys.exit(1)

print "\npET construct len: ", len(plasmid_seqs[0])
print "pRSF construct len: ", len(plasmid_seqs[1]), "\n"
pETDuet_construct_seq_with_re = str(plasmid_seqs[0])
pRSFDuet_construct_seq_with_re = str(plasmid_seqs[1])


# ########## Step # Write all sequences in files


# Write seq of two plasmids in a file
with open(os.path.join(cluster_dir, cluster_name + "_constructs.out.fasta"), "w") as f:
    # with open("constructs.out.fasta", "w") as f:
    f.write(">pETDuet_c\n{}\n>pRSFDuet_c\n{}\n".format(pETDuet_construct_seq_with_re, pRSFDuet_construct_seq_with_re))
f.close()

# Split sequence into Full fragments
#end_index_pET = int(pETDuet_construct_seq_with_re.find(str(elements_to_seq['pETDuet_backbone'])) + len(elements_to_seq['pETDuet_backbone'])) - OVERHANG_LEN
#start_index_ura = pETDuet_construct_seq_with_re.find(str(elements_to_seq['ura3_extended']))
# print "start ura3: ", start_index_ura
#fragment_1 = pETDuet_construct_seq_with_re[end_index_pET: start_index_ura + OVERHANG_LEN]
#print ">Fragment 1\n", fragment_1[:20], fragment_1[-60:], len(fragment_1)
#end_index_ura = start_index_ura + len(elements_to_seq['ura3_extended']) - OVERHANG_LEN
#fragment_2 = pETDuet_construct_seq_with_re[end_index_ura:] + pETDuet_construct_seq_with_re[:OVERHANG_LEN]
#print ">Fragment 2\n", fragment_2[:120], fragment_2[-20:], len(fragment_2)

#end_index_pRSF = int(pRSFDuet_construct_seq_with_re.find(str(elements_to_seq['pRSFDuet_backbone'])) + len(elements_to_seq['pRSFDuet_backbone'])) - OVERHANG_LEN
#start_index_ura = pRSFDuet_construct_seq_with_re.find(str(elements_to_seq['ura3_extended']))
#fragment_3 = pRSFDuet_construct_seq_with_re[end_index_pRSF: start_index_ura + OVERHANG_LEN]
#print ">Fragment 3\n", fragment_3[:20], fragment_3[-60:], len(fragment_3)
#end_index_ura = start_index_ura + len(elements_to_seq['ura3_extended']) - OVERHANG_LEN
#fragment_4 = pRSFDuet_construct_seq_with_re[end_index_ura:] + pRSFDuet_construct_seq_with_re[:OVERHANG_LEN]
#print ">Fragment 4\n", fragment_4[:120], fragment_4[-20:], len(fragment_4)

# Put full fragments in a dictionary
fragment_name_1 = cluster_name + "_" + "Fr1"
fragment_name_2 = cluster_name + "_" + "Fr2"
fragment_name_3 = cluster_name + "_" + "Fr3"
fragment_name_4 = cluster_name + "_" + "Fr4"
#full_fragments = {fragment_name_1: fragment_1, fragment_name_2: fragment_2,
 #                 fragment_name_3: fragment_3, fragment_name_4: fragment_4}

# Write full fragments in a file
#with open(os.path.join(cluster_dir, cluster_name + "_full_fragments.out.fasta"), "w") as f:
#    f.write(">{}\n{}".format(fragment_name_1, fragment_1))
 #   f.write("\n>{}\n{}".format(fragment_name_2, fragment_2))
  #  f.write("\n>{}\n{}".format(fragment_name_3, fragment_3))
  #  f.write("\n>{}\n{}".format(fragment_name_4, fragment_4))
   # f.close()

# Split Full fragments into twist fragments
#w = open(os.path.join(cluster_dir, cluster_name + "_twist_fragments.out.fasta"), "w")
#for fragment_name in full_fragments.keys():
 #   twist_fragments_list = split_seq(full_fragments[fragment_name], fragment_name, TRIM_SIZE, OVERHANG_LEN)
 #   with open(os.path.join(cluster_dir, cluster_name + "_twist_fragments.out.fasta"), "a") as f:
  #      for fragment in twist_fragments_list:
            # print (">%s\n%s\n" % (fragment[0], fragment[1]))
   #         f.write(">%s\n%s\n" % (fragment[0], fragment[1]))

# Write in genbank file
for plasmid in plasmids:
    fasta_to_genbank(plasmid, cluster_name, cluster_dir)
